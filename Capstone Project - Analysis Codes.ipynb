{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'TerraLuna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path):\n",
    "    df=pd.read_csv(path)\n",
    "    if not(df.empty):\n",
    "        df['date']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "        df = df[['id','subreddit', 'selftext', 'title', 'date', 'author']]\n",
    "#         df=df.drop('Unnamed: 0', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terraluna=pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and f.endswith(\".csv\"):\n",
    "        df_terraluna=pd.concat([df_terraluna, process_data(f)])\n",
    "\n",
    "# Drop unneccessary column\n",
    "df_terraluna=df_terraluna.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Reset Index\n",
    "df_terraluna=df_terraluna.reset_index()\n",
    "\n",
    "# Drop Index\n",
    "df_terraluna=df_terraluna.drop('index', axis=1)\n",
    "\n",
    "# Remove [removed]\n",
    "df_terraluna = df_terraluna[df_terraluna.selftext!='[removed]']\n",
    "\n",
    "# Drop dupes\n",
    "df_terraluna = df_terraluna.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_terraluna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terraluna=df_terraluna.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and selftext\n",
    "\n",
    "df_terraluna['all_text'] = df_terraluna.selftext.astype(str) + ' ' + df_terraluna.title.astype(str)\n",
    "df_terraluna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make text lower\n",
    "df_terraluna['all_text'] = df_terraluna['all_text'].astype(str).str.lower()\n",
    "df_terraluna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many submissions per day\n",
    "\n",
    "pd.pivot_table(df_terraluna, index='date', values='all_text', columns='subreddit', aggfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terraluna_comments = pd.read_csv('TerraLuna comments/terraluna_comments.csv', index_col=0,\n",
    "                 lineterminator='\\n')\n",
    "\n",
    "# Clean Removed Comments\n",
    "df_terraluna_comments = df_terraluna_comments[df_terraluna_comments.Comment!='[removed]']\n",
    "\n",
    "# Get cleaned parent ID\n",
    "df_terraluna_comments['parent_id_clean']=df_terraluna_comments['Parent ID'].str[3:]\n",
    "\n",
    "df_terraluna_comments=df_terraluna_comments.rename(columns={\"Submission ID\": \"submission_id\"\n",
    "                                     , \"Parent ID\": \"parent_id\"\n",
    "                                     , \"Comment ID\": \"comment_id\"\n",
    "                                     , \"Comment\": \"comment\"\n",
    "                                     , \"Author\": 'author'}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_terraluna_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terraluna_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA\n",
    "df_terraluna_comments=df_terraluna_comments.fillna('')\n",
    "\n",
    "# Make text lower\n",
    "df_terraluna_comments['comment'] = df_terraluna_comments['comment'].astype(str).str.lower()\n",
    "\n",
    "# Get dates\n",
    "df_terraluna_lookup = df_terraluna[['id','date', 'subreddit']]\n",
    "df_terraluna_lookup_date = df_terraluna_lookup.set_index(\"id\").loc[:, \"date\"]\n",
    "df_terraluna_lookup_subreddit = df_terraluna_lookup.set_index(\"id\").loc[:, \"subreddit\"]\n",
    "df_terraluna_comments=df_terraluna_comments.assign(date=df_terraluna_comments.submission_id.map(df_terraluna_lookup_date))\n",
    "df_terraluna_comments=df_terraluna_comments.assign(subreddit=df_terraluna_comments.submission_id.map(df_terraluna_lookup_subreddit))\n",
    "df_terraluna_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terraluna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns for main data\n",
    "\n",
    "df_terraluna_combine = df_terraluna[['id', 'subreddit', 'all_text', 'date', 'author']]\n",
    "df_terraluna_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terraluna_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns for comments\n",
    "\n",
    "df_terraluna_comments = df_terraluna_comments.rename(columns={'comment': 'all_text', 'comment_id':'id'})\n",
    "df_terraluna_comments_combine = df_terraluna_comments[['id', 'subreddit', 'all_text', 'date', 'author', 'parent_id_clean', 'submission_id']]\n",
    "df_terraluna_comments_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "\n",
    "df_all_combined = pd.concat([df_terraluna_combine, df_terraluna_comments_combine])\n",
    "df_all_combined = df_all_combined.reset_index()\n",
    "df_all_combined = df_all_combined.drop(['index'], axis = 1)\n",
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parent author\n",
    "\n",
    "df_all_combined_lookup = df_all_combined[['id','author']]\n",
    "df_all_combined_lookup = df_all_combined_lookup.drop_duplicates()\n",
    "df_all_combined_lookup.head()\n",
    "\n",
    "df_all_combined_lookup = df_all_combined_lookup.set_index(\"id\").loc[:, \"author\"]\n",
    "\n",
    "df_all_combined=df_all_combined.assign(parent_author=df_all_combined.parent_id_clean.map(df_all_combined_lookup))\n",
    "\n",
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add type of post\n",
    "\n",
    "df_all_combined['type'] = np.nan\n",
    "\n",
    "df_all_combined['type'] = np.where(df_all_combined['parent_id_clean'].isna(), 'submission', 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many submissions per day\n",
    "pd.pivot_table(df_all_combined, index='date', values='all_text', columns=['subreddit', 'type'] , aggfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Package and Regex Tokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regexp = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words meant to be together into one word\n",
    "\n",
    "df_all_combined['all_text'] = df_all_combined['all_text'].str.replace('crypto.com', 'cryptocom')\n",
    "df_all_combined['all_text'] = df_all_combined['all_text'].str.replace('do kwon', 'dokwon')\n",
    "df_all_combined['all_text'] = df_all_combined['all_text'].str.replace(\"do kwon’s\", \"dokwon’s\")\n",
    "\n",
    "# Tokenize all_text\n",
    "\n",
    "df_all_combined['text_token']=df_all_combined['all_text'].apply(regexp.tokenize)\n",
    "\n",
    "# Get English Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Extend the list with custom stopwords\n",
    "my_stopwords = ['https', 'nan', 'removed', 'amp', 'x200b', 'com', 'www', '000']\n",
    "stopwords.extend(my_stopwords)\n",
    "\n",
    "df_all_combined['text_token'] = df_all_combined['text_token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep words with length more than 2\n",
    "\n",
    "df_all_combined['text_string'] = df_all_combined['text_token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all words\n",
    "all_words = ' '.join([word for word in df_all_combined['text_string']])\n",
    "\n",
    "# Tokenize all words\n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of words\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokenized_words)\n",
    "fdist.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get words with occurence of 5 or more\n",
    "\n",
    "df_all_combined['text_string_fdist'] = df_all_combined['text_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] >= 5 ]))\n",
    "df_all_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download for Lemmatization\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lem = WordNetLemmatizer()\n",
    "\n",
    "# get lemmatized words\n",
    "df_all_combined['text_string_lem'] = df_all_combined['text_string_fdist'].apply(wordnet_lem.lemmatize)\n",
    "\n",
    "# check if the columns are equal\n",
    "df_all_combined['is_equal']=(df_all_combined['text_string_fdist']==df_all_combined['text_string_lem'])\n",
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show level count\n",
    "\n",
    "df_all_combined.is_equal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_lem = ' '.join([word for word in df_all_combined['text_string_lem']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "words = nltk.word_tokenize(all_words_lem)\n",
    "fd = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain top 10 words\n",
    "top_10 = fd.most_common(10)\n",
    "\n",
    "# Create pandas series to make plotting easier\n",
    "fdist = pd.Series(dict(top_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "sns.barplot(y=fdist.index, x=fdist.values, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission Data\n",
    "\n",
    "# Using Polarity\n",
    "df_all_combined['polarity'] = df_all_combined['text_string_lem'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "df_all_combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data structure\n",
    "df_all_combined = pd.concat(\n",
    "    [df_all_combined, \n",
    "     df_all_combined['polarity'].apply(pd.Series)], axis=1)\n",
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new sentiment variable\n",
    "df_all_combined['sentiment'] = df_all_combined['compound'].apply(lambda x: 'positive' if x >0 else 'neutral' if x==0 else 'negative')\n",
    "df_all_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of submissions/comments by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined, \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of submissions by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined[df_all_combined.type=='submission'], \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of comments by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined[df_all_combined.type=='comment'], \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many submissions per day\n",
    "pd.pivot_table(df_all_combined, index='date', values='all_text', columns=['sentiment', 'type'] , aggfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all edges \n",
    "\n",
    "df_all_combined_networkx_edges = df_all_combined[['author', 'parent_author']]\n",
    "df_all_combined_networkx_edges = df_all_combined_networkx_edges[df_all_combined_networkx_edges.parent_author.notna()]\n",
    "\n",
    "df_all_combined_networkx_edges = df_all_combined_networkx_edges[df_all_combined_networkx_edges.author!='ccModBot']\n",
    "df_all_combined_networkx_edges = df_all_combined_networkx_edges[df_all_combined_networkx_edges.parent_author!='ccModBot']\n",
    "\n",
    "df_all_combined_networkx_edges = df_all_combined_networkx_edges[df_all_combined_networkx_edges.author!='AutoModerator']\n",
    "df_all_combined_networkx_edges = df_all_combined_networkx_edges[df_all_combined_networkx_edges.parent_author!='AutoModerator']\n",
    "\n",
    "df_all_combined_networkx_edges = df_all_combined_networkx_edges[df_all_combined_networkx_edges.author!='']\n",
    "df_all_combined_networkx_edges = df_all_combined_networkx_edges[df_all_combined_networkx_edges.parent_author!='']\n",
    "\n",
    "df_all_combined_networkx_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all nodes \n",
    "\n",
    "df_all_combined_networkx_nodes = df_all_combined[['author']]\n",
    "df_all_combined_networkx_nodes = df_all_combined_networkx_nodes.drop_duplicates()\n",
    "\n",
    "df_all_combined_networkx_nodes = df_all_combined_networkx_nodes[df_all_combined_networkx_nodes.author!='ccModBot']\n",
    "df_all_combined_networkx_nodes = df_all_combined_networkx_nodes[df_all_combined_networkx_nodes.author!='AutoModerator']\n",
    "df_all_combined_networkx_nodes = df_all_combined_networkx_nodes[df_all_combined_networkx_nodes.author!='']\n",
    "\n",
    "df_all_combined_networkx_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_all_combined_networkx_edges))\n",
    "print(len(df_all_combined_networkx_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try add weights from frequency of edges\n",
    "\n",
    "df_all_combined_networkx_edges['weight'] = df_all_combined_networkx_edges.groupby(['author', 'parent_author'])['author'].transform('size')\n",
    "df_all_combined_networkx_edges.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to the graph\n",
    "for author in df_all_combined_networkx_nodes.author:\n",
    "    G.add_node(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges using pandas, to incorporate weights\n",
    "\n",
    "G = nx.from_pandas_edgelist(df_all_combined_networkx_edges, 'author', 'parent_author',\n",
    "                            create_using=nx.Graph(), edge_attr='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of nodes and edges in the graph\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Print the most central nodes based on degree centrality\n",
    "print(\"Top 5 nodes based on degree centrality:\")\n",
    "sorted_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "for node, centrality in sorted_degree_centrality[:5]:\n",
    "    print(f\"{node}: {centrality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures 2\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Print the most central nodes based on closeness centrality\n",
    "print(\"Top 5 nodes based on closeness centrality:\")\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "for node, centrality in sorted_closeness_centrality[:5]:\n",
    "    print(f\"{node}: {centrality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures 3\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Print the most central nodes based on betweenness centrality\n",
    "print(\"Top 5 nodes based on betweenness centrality:\")\n",
    "sorted_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "for node, centrality in sorted_betweenness_centrality[:5]:\n",
    "    print(f\"{node}: {centrality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures 4\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "# Print the most central nodes based on eigen centrality\n",
    "print(\"Top 5 nodes based on eigen centrality:\")\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "for node, centrality in sorted_eigenvector_centrality[:5]:\n",
    "    print(f\"{node}: {centrality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only nodes with highest eigen centrality\n",
    "\n",
    "sorted_eigenvector_centrality_2 = sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)\n",
    "top_nodes = sorted_eigenvector_centrality_2[:1]  # Select top node only\n",
    "\n",
    "# top_nodes_and_neighbors = []\n",
    "\n",
    "# for node in top_nodes:\n",
    "#     for nodex in G.neighbors(node):\n",
    "#         top_nodes_and_neighbors.append(nodex)\n",
    "#         top_nodes_and_neighbors = list(set(top_nodes_and_neighbors))\n",
    "        \n",
    "# subgraph = G.subgraph(top_nodes_and_neighbors)\n",
    "subgraph = G.subgraph(top_nodes)\n",
    "\n",
    "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\n",
    "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\n",
    "\n",
    "pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n",
    ")\n",
    "\n",
    "# node labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
    "# edge weight labels\n",
    "edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Five Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined_phase1 = df_all_combined[df_all_combined.date<=datetime.datetime.strptime('2022-05-06', '%Y-%m-%d').date()]\n",
    "df_all_combined_phase2 = df_all_combined[(df_all_combined.date>=datetime.datetime.strptime('2022-05-07', '%Y-%m-%d').date()) & (df_all_combined.date<=datetime.datetime.strptime('2022-05-09', '%Y-%m-%d').date())]\n",
    "df_all_combined_phase3 = df_all_combined[(df_all_combined.date>=datetime.datetime.strptime('2022-05-10', '%Y-%m-%d').date()) & (df_all_combined.date<=datetime.datetime.strptime('2022-05-13', '%Y-%m-%d').date())]\n",
    "df_all_combined_phase4 = df_all_combined[(df_all_combined.date>=datetime.datetime.strptime('2022-05-14', '%Y-%m-%d').date()) & (df_all_combined.date<=datetime.datetime.strptime('2022-05-28', '%Y-%m-%d').date())]\n",
    "df_all_combined_phase5 = df_all_combined[(df_all_combined.date>=datetime.datetime.strptime('2022-05-29', '%Y-%m-%d').date())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of submissions/comments by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined_phase1, \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of submissions/comments by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined_phase2, \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of submissions/comments by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined_phase3, \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of submissions/comments by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined_phase4, \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of submissions/comments by sentiment\n",
    "sns.countplot(y='sentiment', \n",
    "             data=df_all_combined_phase5, \n",
    "             order=['positive', 'neutral', 'negative'], \n",
    "             palette=['#b2d8d8',\"#008080\", '#db3d13']\n",
    "             );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all edges \n",
    "\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_phase1[['author', 'parent_author']]\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_networkx_edges_phase1[df_all_combined_networkx_edges_phase1.parent_author.notna()]\n",
    "\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_networkx_edges_phase1[df_all_combined_networkx_edges_phase1.author!='ccModBot']\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_networkx_edges_phase1[df_all_combined_networkx_edges_phase1.parent_author!='ccModBot']\n",
    "\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_networkx_edges_phase1[df_all_combined_networkx_edges_phase1.author!='AutoModerator']\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_networkx_edges_phase1[df_all_combined_networkx_edges_phase1.parent_author!='AutoModerator']\n",
    "\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_networkx_edges_phase1[df_all_combined_networkx_edges_phase1.author!='']\n",
    "df_all_combined_networkx_edges_phase1 = df_all_combined_networkx_edges_phase1[df_all_combined_networkx_edges_phase1.parent_author!='']\n",
    "\n",
    "# Get all nodes \n",
    "\n",
    "df_all_combined_networkx_nodes_phase1 = df_all_combined_phase1[['author']]\n",
    "df_all_combined_networkx_nodes_phase1 = df_all_combined_networkx_nodes_phase1.drop_duplicates()\n",
    "\n",
    "df_all_combined_networkx_nodes_phase1 = df_all_combined_networkx_nodes_phase1[df_all_combined_networkx_nodes_phase1.author!='ccModBot']\n",
    "df_all_combined_networkx_nodes_phase1 = df_all_combined_networkx_nodes_phase1[df_all_combined_networkx_nodes_phase1.author!='AutoModerator']\n",
    "df_all_combined_networkx_nodes_phase1 = df_all_combined_networkx_nodes_phase1[df_all_combined_networkx_nodes_phase1.author!='']\n",
    "\n",
    "# Try add weights from frequency of edges\n",
    "\n",
    "df_all_combined_networkx_edges_phase1['weight'] = df_all_combined_networkx_edges_phase1.groupby(['author', 'parent_author'])['author'].transform('size')\n",
    "df_all_combined_networkx_edges_phase1.head()\n",
    "\n",
    "print(len(df_all_combined_networkx_edges_phase1))\n",
    "print(len(df_all_combined_networkx_nodes_phase1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all edges \n",
    "\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_phase2[['author', 'parent_author']]\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_networkx_edges_phase2[df_all_combined_networkx_edges_phase2.parent_author.notna()]\n",
    "\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_networkx_edges_phase2[df_all_combined_networkx_edges_phase2.author!='ccModBot']\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_networkx_edges_phase2[df_all_combined_networkx_edges_phase2.parent_author!='ccModBot']\n",
    "\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_networkx_edges_phase2[df_all_combined_networkx_edges_phase2.author!='AutoModerator']\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_networkx_edges_phase2[df_all_combined_networkx_edges_phase2.parent_author!='AutoModerator']\n",
    "\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_networkx_edges_phase2[df_all_combined_networkx_edges_phase2.author!='']\n",
    "df_all_combined_networkx_edges_phase2 = df_all_combined_networkx_edges_phase2[df_all_combined_networkx_edges_phase2.parent_author!='']\n",
    "\n",
    "# Get all nodes \n",
    "\n",
    "df_all_combined_networkx_nodes_phase2 = df_all_combined_phase2[['author']]\n",
    "df_all_combined_networkx_nodes_phase2 = df_all_combined_networkx_nodes_phase2.drop_duplicates()\n",
    "\n",
    "df_all_combined_networkx_nodes_phase2 = df_all_combined_networkx_nodes_phase2[df_all_combined_networkx_nodes_phase2.author!='ccModBot']\n",
    "df_all_combined_networkx_nodes_phase2 = df_all_combined_networkx_nodes_phase2[df_all_combined_networkx_nodes_phase2.author!='AutoModerator']\n",
    "df_all_combined_networkx_nodes_phase2 = df_all_combined_networkx_nodes_phase2[df_all_combined_networkx_nodes_phase2.author!='']\n",
    "\n",
    "# Try add weights from frequency of edges\n",
    "\n",
    "df_all_combined_networkx_edges_phase2['weight'] = df_all_combined_networkx_edges_phase2.groupby(['author', 'parent_author'])['author'].transform('size')\n",
    "df_all_combined_networkx_edges_phase2.head()\n",
    "\n",
    "print(len(df_all_combined_networkx_edges_phase2))\n",
    "print(len(df_all_combined_networkx_nodes_phase2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all edges \n",
    "\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_phase3[['author', 'parent_author']]\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_networkx_edges_phase3[df_all_combined_networkx_edges_phase3.parent_author.notna()]\n",
    "\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_networkx_edges_phase3[df_all_combined_networkx_edges_phase3.author!='ccModBot']\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_networkx_edges_phase3[df_all_combined_networkx_edges_phase3.parent_author!='ccModBot']\n",
    "\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_networkx_edges_phase3[df_all_combined_networkx_edges_phase3.author!='AutoModerator']\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_networkx_edges_phase3[df_all_combined_networkx_edges_phase3.parent_author!='AutoModerator']\n",
    "\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_networkx_edges_phase3[df_all_combined_networkx_edges_phase3.author!='']\n",
    "df_all_combined_networkx_edges_phase3 = df_all_combined_networkx_edges_phase3[df_all_combined_networkx_edges_phase3.parent_author!='']\n",
    "\n",
    "# Get all nodes \n",
    "\n",
    "df_all_combined_networkx_nodes_phase3 = df_all_combined_phase3[['author']]\n",
    "df_all_combined_networkx_nodes_phase3 = df_all_combined_networkx_nodes_phase3.drop_duplicates()\n",
    "\n",
    "df_all_combined_networkx_nodes_phase3 = df_all_combined_networkx_nodes_phase3[df_all_combined_networkx_nodes_phase3.author!='ccModBot']\n",
    "df_all_combined_networkx_nodes_phase3 = df_all_combined_networkx_nodes_phase3[df_all_combined_networkx_nodes_phase3.author!='AutoModerator']\n",
    "df_all_combined_networkx_nodes_phase3 = df_all_combined_networkx_nodes_phase3[df_all_combined_networkx_nodes_phase3.author!='']\n",
    "\n",
    "# Try add weights from frequency of edges\n",
    "\n",
    "df_all_combined_networkx_edges_phase3['weight'] = df_all_combined_networkx_edges_phase3.groupby(['author', 'parent_author'])['author'].transform('size')\n",
    "df_all_combined_networkx_edges_phase3.head()\n",
    "\n",
    "print(len(df_all_combined_networkx_edges_phase3))\n",
    "print(len(df_all_combined_networkx_nodes_phase3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all edges \n",
    "\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_phase4[['author', 'parent_author']]\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_networkx_edges_phase4[df_all_combined_networkx_edges_phase4.parent_author.notna()]\n",
    "\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_networkx_edges_phase4[df_all_combined_networkx_edges_phase4.author!='ccModBot']\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_networkx_edges_phase4[df_all_combined_networkx_edges_phase4.parent_author!='ccModBot']\n",
    "\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_networkx_edges_phase4[df_all_combined_networkx_edges_phase4.author!='AutoModerator']\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_networkx_edges_phase4[df_all_combined_networkx_edges_phase4.parent_author!='AutoModerator']\n",
    "\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_networkx_edges_phase4[df_all_combined_networkx_edges_phase4.author!='']\n",
    "df_all_combined_networkx_edges_phase4 = df_all_combined_networkx_edges_phase4[df_all_combined_networkx_edges_phase4.parent_author!='']\n",
    "\n",
    "# Get all nodes \n",
    "\n",
    "df_all_combined_networkx_nodes_phase4 = df_all_combined_phase4[['author']]\n",
    "df_all_combined_networkx_nodes_phase4 = df_all_combined_networkx_nodes_phase4.drop_duplicates()\n",
    "\n",
    "df_all_combined_networkx_nodes_phase4 = df_all_combined_networkx_nodes_phase4[df_all_combined_networkx_nodes_phase4.author!='ccModBot']\n",
    "df_all_combined_networkx_nodes_phase4 = df_all_combined_networkx_nodes_phase4[df_all_combined_networkx_nodes_phase4.author!='AutoModerator']\n",
    "df_all_combined_networkx_nodes_phase4 = df_all_combined_networkx_nodes_phase4[df_all_combined_networkx_nodes_phase4.author!='']\n",
    "\n",
    "# Try add weights from frequency of edges\n",
    "\n",
    "df_all_combined_networkx_edges_phase4['weight'] = df_all_combined_networkx_edges_phase4.groupby(['author', 'parent_author'])['author'].transform('size')\n",
    "df_all_combined_networkx_edges_phase4.head()\n",
    "\n",
    "print(len(df_all_combined_networkx_edges_phase4))\n",
    "print(len(df_all_combined_networkx_nodes_phase4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all edges \n",
    "\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_phase5[['author', 'parent_author']]\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_networkx_edges_phase5[df_all_combined_networkx_edges_phase5.parent_author.notna()]\n",
    "\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_networkx_edges_phase5[df_all_combined_networkx_edges_phase5.author!='ccModBot']\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_networkx_edges_phase5[df_all_combined_networkx_edges_phase5.parent_author!='ccModBot']\n",
    "\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_networkx_edges_phase5[df_all_combined_networkx_edges_phase5.author!='AutoModerator']\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_networkx_edges_phase5[df_all_combined_networkx_edges_phase5.parent_author!='AutoModerator']\n",
    "\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_networkx_edges_phase5[df_all_combined_networkx_edges_phase5.author!='']\n",
    "df_all_combined_networkx_edges_phase5 = df_all_combined_networkx_edges_phase5[df_all_combined_networkx_edges_phase5.parent_author!='']\n",
    "\n",
    "# Get all nodes \n",
    "\n",
    "df_all_combined_networkx_nodes_phase5 = df_all_combined_phase5[['author']]\n",
    "df_all_combined_networkx_nodes_phase5 = df_all_combined_networkx_nodes_phase5.drop_duplicates()\n",
    "\n",
    "df_all_combined_networkx_nodes_phase5 = df_all_combined_networkx_nodes_phase5[df_all_combined_networkx_nodes_phase5.author!='ccModBot']\n",
    "df_all_combined_networkx_nodes_phase5 = df_all_combined_networkx_nodes_phase5[df_all_combined_networkx_nodes_phase5.author!='AutoModerator']\n",
    "df_all_combined_networkx_nodes_phase5 = df_all_combined_networkx_nodes_phase5[df_all_combined_networkx_nodes_phase5.author!='']\n",
    "\n",
    "# Try add weights from frequency of edges\n",
    "\n",
    "df_all_combined_networkx_edges_phase5['weight'] = df_all_combined_networkx_edges_phase5.groupby(['author', 'parent_author'])['author'].transform('size')\n",
    "df_all_combined_networkx_edges_phase5.head()\n",
    "\n",
    "print(len(df_all_combined_networkx_edges_phase5))\n",
    "print(len(df_all_combined_networkx_nodes_phase5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "G_phase1 = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for author in df_all_combined_networkx_nodes_phase1.author:\n",
    "    G_phase1.add_node(author)\n",
    "    \n",
    "# Add edges using pandas, to incorporate weights\n",
    "\n",
    "G_phase1 = nx.from_pandas_edgelist(df_all_combined_networkx_edges_phase1, 'author', 'parent_author',\n",
    "                            create_using=nx.Graph(), edge_attr='weight')\n",
    "\n",
    "# Print the number of nodes and edges in the graph\n",
    "print(\"Number of nodes:\", G_phase1.number_of_nodes())\n",
    "print(\"Number of edges:\", G_phase1.number_of_edges())\n",
    "\n",
    "# degree_centrality_phase1 = nx.degree_centrality(G_phase1)\n",
    "# closeness_centrality_phase1 = nx.closeness_centrality(G_phase1)\n",
    "# betweenness_centrality_phase1 = nx.betweenness_centrality(G_phase1)\n",
    "eigenvector_centrality_phase1 = nx.eigenvector_centrality(G_phase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "G_phase2 = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for author in df_all_combined_networkx_nodes_phase2.author:\n",
    "    G_phase2.add_node(author)\n",
    "    \n",
    "# Add edges using pandas, to incorporate weights\n",
    "\n",
    "G_phase2 = nx.from_pandas_edgelist(df_all_combined_networkx_edges_phase2, 'author', 'parent_author',\n",
    "                            create_using=nx.Graph(), edge_attr='weight')\n",
    "\n",
    "# Print the number of nodes and edges in the graph\n",
    "print(\"Number of nodes:\", G_phase2.number_of_nodes())\n",
    "print(\"Number of edges:\", G_phase2.number_of_edges())\n",
    "\n",
    "# degree_centrality_phase2 = nx.degree_centrality(G_phase2)\n",
    "# closeness_centrality_phase2 = nx.closeness_centrality(G_phase2)\n",
    "# betweenness_centrality_phase2 = nx.betweenness_centrality(G_phase2)\n",
    "eigenvector_centrality_phase2 = nx.eigenvector_centrality(G_phase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "G_phase3 = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for author in df_all_combined_networkx_nodes_phase3.author:\n",
    "    G_phase3.add_node(author)\n",
    "    \n",
    "# Add edges using pandas, to incorporate weights\n",
    "\n",
    "G_phase3 = nx.from_pandas_edgelist(df_all_combined_networkx_edges_phase3, 'author', 'parent_author',\n",
    "                            create_using=nx.Graph(), edge_attr='weight')\n",
    "\n",
    "# Print the number of nodes and edges in the graph\n",
    "print(\"Number of nodes:\", G_phase3.number_of_nodes())\n",
    "print(\"Number of edges:\", G_phase3.number_of_edges())\n",
    "\n",
    "# degree_centrality_phase3 = nx.degree_centrality(G_phase3)\n",
    "# closeness_centrality_phase3 = nx.closeness_centrality(G_phase3)\n",
    "# betweenness_centrality_phase3 = nx.betweenness_centrality(G_phase3)\n",
    "eigenvector_centrality_phase3 = nx.eigenvector_centrality(G_phase3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "G_phase4 = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for author in df_all_combined_networkx_nodes_phase4.author:\n",
    "    G_phase4.add_node(author)\n",
    "    \n",
    "# Add edges using pandas, to incorporate weights\n",
    "\n",
    "G_phase4 = nx.from_pandas_edgelist(df_all_combined_networkx_edges_phase4, 'author', 'parent_author',\n",
    "                            create_using=nx.Graph(), edge_attr='weight')\n",
    "\n",
    "# Print the number of nodes and edges in the graph\n",
    "print(\"Number of nodes:\", G_phase4.number_of_nodes())\n",
    "print(\"Number of edges:\", G_phase4.number_of_edges())\n",
    "\n",
    "# degree_centrality_phase4 = nx.degree_centrality(G_phase4)\n",
    "# closeness_centrality_phase4 = nx.closeness_centrality(G_phase4)\n",
    "# betweenness_centrality_phase4 = nx.betweenness_centrality(G_phase4)\n",
    "eigenvector_centrality_phase4 = nx.eigenvector_centrality(G_phase4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "G_phase5 = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for author in df_all_combined_networkx_nodes_phase5.author:\n",
    "    G_phase5.add_node(author)\n",
    "    \n",
    "# Add edges using pandas, to incorporate weights\n",
    "\n",
    "G_phase5 = nx.from_pandas_edgelist(df_all_combined_networkx_edges_phase5, 'author', 'parent_author',\n",
    "                            create_using=nx.Graph(), edge_attr='weight')\n",
    "\n",
    "# Print the number of nodes and edges in the graph\n",
    "print(\"Number of nodes:\", G_phase5.number_of_nodes())\n",
    "print(\"Number of edges:\", G_phase5.number_of_edges())\n",
    "\n",
    "# degree_centrality_phase5 = nx.degree_centrality(G_phase5)\n",
    "# closeness_centrality_phase5 = nx.closeness_centrality(G_phase5)\n",
    "# betweenness_centrality_phase5 = nx.betweenness_centrality(G_phase5)\n",
    "eigenvector_centrality_phase5 = nx.eigenvector_centrality(G_phase5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Nodes with High EigenCentrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eigenvector_centrality_phase1 = sorted(eigenvector_centrality_phase1.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_eigenvector_centrality_phase2 = sorted(eigenvector_centrality_phase2.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_eigenvector_centrality_phase3 = sorted(eigenvector_centrality_phase3.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_eigenvector_centrality_phase4 = sorted(eigenvector_centrality_phase4.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_eigenvector_centrality_phase5 = sorted(eigenvector_centrality_phase5.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 5 nodes based on eigencentrality, total:\")\n",
    "for node, centrality in sorted_eigenvector_centrality[:5]:\n",
    "    print(\"User '\" + node + \"' posted \" + str(len(df_all_combined[df_all_combined.author==node])) + \n",
    "          \" times, with eigencentrality score of: \" + str(centrality))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, centrality in sorted_eigenvector_centrality[:5]:\n",
    "    print(\"All submissions/comments posted by user '\" + node + \"' has net sentiment value of: \" \n",
    "          + f\"{np.average(df_all_combined[df_all_combined.author==node].pos - df_all_combined[df_all_combined.author==node].neg):.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_topics(bow_corpus, dictionary):\n",
    "    coherence_dict={}\n",
    "    for num_topics in range(1,11):\n",
    "    \n",
    "        lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "        coherence_model = CoherenceModel(model=lda_model, texts=tokenized_corpus, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "        # print the coherence score\n",
    "        coherence_dict[num_topics] = coherence_score\n",
    "        \n",
    "    best_num_topics = max(coherence_dict, key=coherence_dict.get)\n",
    "    return best_num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_by_user(df_all_combined, node):\n",
    "    # Sample text corpus\n",
    "    corpus = df_all_combined[df_all_combined.author==node].text_string_lem\n",
    "\n",
    "    # Preprocessing the corpus\n",
    "    tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "\n",
    "    # Replace TerraLuna related words in corpus with a single word/category\n",
    "    original_list = tokenized_corpus\n",
    "    replacement_value = \"terraluna\"\n",
    "    to_be_replaced = ['luna', 'terra', 'ust', 'classic', 'lunc', 'anchor']\n",
    "\n",
    "    new_list=[]\n",
    "\n",
    "    for nest_list in original_list:\n",
    "        new_nest_list=[]\n",
    "        for _ in nest_list:\n",
    "            if _ in to_be_replaced:\n",
    "                new_nest_list.append(replacement_value)\n",
    "            else: \n",
    "                new_nest_list.append(_)\n",
    "        new_list.append(new_nest_list)\n",
    "\n",
    "    tokenized_corpus = new_list\n",
    "\n",
    "    # Delete words\n",
    "    original_list = tokenized_corpus\n",
    "\n",
    "    to_be_deleted = ['amp', 'com', 'x200b', 'www', '000', '100', 'd0aqga2jlmypxcg', 'swjsh8']\n",
    "\n",
    "    new_list=[]\n",
    "\n",
    "    for nest_list in original_list:\n",
    "        new_nest_list=[]\n",
    "        for _ in nest_list:\n",
    "            if _ in to_be_deleted:\n",
    "                pass\n",
    "            else: \n",
    "                new_nest_list.append(_)\n",
    "        new_list.append(new_nest_list)\n",
    "\n",
    "    tokenized_corpus = new_list\n",
    "\n",
    "    # Create a dictionary from the tokenized corpus\n",
    "    dictionary = corpora.Dictionary(tokenized_corpus)\n",
    "\n",
    "    # Convert the tokenized corpus to a bag-of-words representation\n",
    "    bow_corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_corpus]\n",
    "\n",
    "    # Use no of topics as per trained \n",
    "    lda_model = LdaModel(bow_corpus, num_topics=get_num_topics(bow_corpus, dictionary), id2word=dictionary, passes=10)\n",
    "\n",
    "    print(\"User '\" + node + \"' posted \" + str(len(df_all_combined[df_all_combined.author==node])) + \n",
    "          \" times, with eigencentrality score of: \" + str(centrality) + \"\\n\")\n",
    "    \n",
    "    print(\"All submissions/comments posted by user '\" + node + \"' has net sentiment value of: \" \n",
    "          + f\"{np.average(df_all_combined[df_all_combined.author==node].pos - df_all_combined[df_all_combined.author==node].neg):.1%}\" + \"\\n\")\n",
    "\n",
    "    print(f\"For user \" + node + \" here are the \"+ str(get_num_topics(bow_corpus, dictionary)) +\" topics of the submissions/comments posted by them:\\n\")\n",
    "    \n",
    "    # Print the topics and their corresponding words\n",
    "    for topic_id in range(num_topics_final):\n",
    "        print(f\"Topic {topic_id + 1}:\")\n",
    "        words = lda_model.show_topic(topic_id)\n",
    "        for word, prob in words:\n",
    "            print(f\"{word}: {prob}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, centrality in sorted_eigenvector_centrality[:5]:\n",
    "    lda_by_user(df_all_combined, node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Users by Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, centrality in sorted_eigenvector_centrality_phase1[:5]:\n",
    "    lda_by_user(df_all_combined_phase1, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, centrality in sorted_eigenvector_centrality_phase2[:5]:\n",
    "    lda_by_user(df_all_combined_phase2, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, centrality in sorted_eigenvector_centrality_phase3[:5]:\n",
    "    lda_by_user(df_all_combined_phase3, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, centrality in sorted_eigenvector_centrality_phase4[:5]:\n",
    "    lda_by_user(df_all_combined_phase4, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, centrality in sorted_eigenvector_centrality_phase5[:5]:\n",
    "    lda_by_user(df_all_combined_phase5, node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
